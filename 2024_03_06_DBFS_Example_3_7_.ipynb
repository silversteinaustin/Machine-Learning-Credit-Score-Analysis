{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silversteinaustin/Machine-Learning-Credit-Score-Analysis/blob/main/2024_03_06_DBFS_Example_3_7_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
          "showTitle": false,
          "title": ""
        },
        "id": "wpu03pInTLTi"
      },
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
        "\n",
        "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bec92824-3fff-4309-98b1-9607a68c6902",
          "showTitle": false,
          "title": ""
        },
        "id": "895K69S4TLTk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score # AUC ROC\n",
        "from sklearn.metrics import average_precision_score # AUC PRC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YourAppName\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUu9W9B7UMiP",
        "outputId": "09cc99d8-f419-4417-b8fc-27c87c1c8632"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "psTNg6ikTLTj",
        "outputId": "ee5cd607-2e8d-4288-ff4b-ebb38c74a723"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Id: int, Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int, Risk_Flag: int]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# File location and type\n",
        "file_location = \"/content/Training Data (1).csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "loan_df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(loan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "713d8411-5fd8-4fe0-9127-bd2739d7a6a8",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNZzn2AoTLTk",
        "outputId": "fa9c9a16-ec9b-4d63-dcb4-cc5897aeb2d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Id: int, Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int, Risk_Flag: int]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "loan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ebd794a-3df0-4658-8eba-11d1f592fa9c",
          "showTitle": false,
          "title": ""
        },
        "id": "T8APv5CETLTk"
      },
      "outputs": [],
      "source": [
        "#dropped the unnecessary data from out dataframe\n",
        "\n",
        "clean_df = loan_df.drop('ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7594663f-a112-4685-b5da-fab63b853381",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MsDAy0B5TLTk",
        "outputId": "3529c504-b962-40d4-bcf5-2e7a8f4abef4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int, Risk_Flag: int]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(clean_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e23ca235-d142-4f66-8354-7f726a2955e4",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKhacWoqTLTl",
        "outputId": "4d6b0e2a-5305-450d-aa5f-388e099ee934"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "clean_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ef3f5927-3e18-46f8-a6b6-5942f2117cbc",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCEsp4yVTLTl",
        "outputId": "afc01192-0fa9-405d-bdc2-6c816fd27c5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "super_clean_df = clean_df.dropna()\n",
        "super_clean_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "06f7d23e-c76a-4b9b-8814-80ad994f5533",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Zm8dN6TLTl",
        "outputId": "5877c04a-70ee-40cc-d375-6d6fcc9be9c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Income', 'Age', 'Experience', 'Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS', 'Risk_Flag']\n"
          ]
        }
      ],
      "source": [
        "column_list = super_clean_df.columns\n",
        "print(column_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "30841d99-2cfb-47af-a39e-2c59cd8ca753",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlUIdNlhTLTl",
        "outputId": "4100be4c-fd39-4ab6-866f-917bb75f8177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norent_noown\n",
            "rented\n",
            "owned\n"
          ]
        }
      ],
      "source": [
        "# Check the unique values in the House_Ownership column\n",
        "unique_house_ownership_values = super_clean_df.select('House_Ownership').distinct().collect()\n",
        "\n",
        "# Print the unique values\n",
        "for row in unique_house_ownership_values:\n",
        "    print(row['House_Ownership'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "847570d9-54dd-4b43-be6d-887f7579edfe",
          "showTitle": false,
          "title": ""
        },
        "id": "5Tg7lhLtTLTl"
      },
      "outputs": [],
      "source": [
        "# Define custom mapping for House_Ownership column\n",
        "house_ownership_mapping = {\"norent_noown\": 0, \"rented\": 1, \"owned\": 2}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1356c1dc-5ec9-4e24-a0ae-c134e2fdb820",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7gdQ3m5TLTm",
        "outputId": "878b6bbc-1ae7-41e6-c686-83536860a258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Extract the 'Risk_Flag' column from the DataFrame and convert it to a numpy array\n",
        "y = np.array(super_clean_df.select(\"Risk_Flag\").collect())\n",
        "\n",
        "# Drop the 'Risk_Flag' column from the DataFrame to get the feature matrix\n",
        "X = super_clean_df.drop(\"Risk_Flag\")\n",
        "\n",
        "# Display the first 5 elements of y\n",
        "print(y[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_list = super_clean_df.columns\n",
        "print(column_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuN7EcEnpW_L",
        "outputId": "5985120a-05df-4d9e-ead4-5dc6e1be0351"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Income', 'Age', 'Experience', 'Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS', 'Risk_Flag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPWojQo6graZ",
        "outputId": "f9559b29-ad64-49be-c4b9-965f446094c4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+\n",
            "| Income|Age|Experience|Married/Single|House_Ownership|Car_Ownership|          Profession|               CITY|         STATE|CURRENT_JOB_YRS|CURRENT_HOUSE_YRS|\n",
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+\n",
            "|1303834| 23|         3|        single|         rented|           no| Mechanical_engineer|               Rewa|Madhya_Pradesh|              3|               13|\n",
            "|7574516| 40|        10|        single|         rented|           no|  Software_Developer|           Parbhani|   Maharashtra|              9|               13|\n",
            "|3991815| 66|         4|       married|         rented|           no|    Technical_writer|          Alappuzha|        Kerala|              4|               10|\n",
            "|6256451| 41|         2|        single|         rented|          yes|  Software_Developer|        Bhubaneswar|        Odisha|              2|               12|\n",
            "|5768871| 47|        11|        single|         rented|           no|       Civil_servant|Tiruchirappalli[10]|    Tamil_Nadu|              3|               14|\n",
            "|6915937| 64|         0|        single|         rented|           no|       Civil_servant|            Jalgaon|   Maharashtra|              0|               12|\n",
            "|3954973| 58|        14|       married|         rented|           no|           Librarian|           Tiruppur|    Tamil_Nadu|              8|               12|\n",
            "|1706172| 33|         2|        single|         rented|           no|           Economist|           Jamnagar|       Gujarat|              2|               14|\n",
            "|7566849| 24|        17|        single|         rented|          yes|    Flight_attendant|            Kota[6]|     Rajasthan|             11|               11|\n",
            "|8964846| 23|        12|        single|         rented|           no|           Architect|         Karimnagar|     Telangana|              5|               13|\n",
            "|4634680| 78|         7|        single|         rented|           no|    Flight_attendant|        Hajipur[31]|         Bihar|              7|               12|\n",
            "|6623263| 22|         4|        single|         rented|           no|            Designer|              Adoni|Andhra_Pradesh|              4|               14|\n",
            "|9120988| 28|         9|        single|         rented|           no|           Physician|          Erode[17]|    Tamil_Nadu|              9|               12|\n",
            "|8043880| 57|        12|        single|         rented|           no|   Financial_Analyst|             Kollam|        Kerala|              8|               10|\n",
            "|9420838| 48|         6|        single|         rented|           no|    Technical_writer|            Madurai|    Tamil_Nadu|              6|               10|\n",
            "|5694236| 39|         2|       married|         rented|          yes|           Economist|    Anantapuram[24]|Andhra_Pradesh|              2|               10|\n",
            "|7315840| 71|         8|       married|         rented|           no|Air_traffic_contr...|          Kamarhati|   West_Bengal|              8|               14|\n",
            "|3666346| 56|        12|        single|         rented|           no|          Politician|           Bhusawal|   Maharashtra|             12|               11|\n",
            "|2241112| 28|         8|        single|         rented|           no|      Police_officer|              Sirsa|       Haryana|              6|               14|\n",
            "|5431918| 40|         1|        single|         rented|           no|              Artist|          Amaravati|Andhra_Pradesh|              1|               14|\n",
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxHvPdmEkdHS",
        "outputId": "4210c606-b93f-425e-aa3b-1df546b904d2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "eb2a4df6-b14a-4df3-8d2e-7a029401d2e9",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "o10NvEXvTLTl",
        "outputId": "7b2d8a80-2155-4e5d-c1ad-039659d72278"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int]\n",
              "0                                                  1                                                                                                                                                                       "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea84a702-5f67-4bb1-b36a-676543e0f352\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea84a702-5f67-4bb1-b36a-676543e0f352')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea84a702-5f67-4bb1-b36a-676543e0f352 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea84a702-5f67-4bb1-b36a-676543e0f352');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_2fed05c8-ff90-4f91-a9a0-647ccbc0a3f3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_dummies')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2fed05c8-ff90-4f91-a9a0-647ccbc0a3f3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_dummies');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_dummies",
              "summary": "{\n  \"name\": \"X_dummies\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int]\",\n      \"properties\": {\n        \"dtype\": \"uint8\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "X_dummies = pd.get_dummies(X)\n",
        "X_dummies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_cols = ['Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE']\n",
        "\n",
        "# Index categorical columns\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "\n",
        "# One-hot encode indexed categorical columns\n",
        "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_encoded\") for col in categorical_cols]\n",
        "\n",
        "# Pipeline of indexers and encoders\n",
        "pipeline = Pipeline(stages=indexers + encoders)\n",
        "\n",
        "# Fit and transform the pipeline to create dummy variables\n",
        "X_encoded = pipeline.fit(X).transform(X)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "X_encoded = X_encoded.drop(*[col+\"_index\" for col in categorical_cols])\n",
        "\n",
        "# Show the resulting DataFrame with dummy variables\n",
        "X_encoded.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK74K-JXlDxk",
        "outputId": "56a53ebf-12c4-4030-d218-65d1f39fe5cc"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+----------------------+-----------------------+---------------------+------------------+-----------------+---------------+\n",
            "| Income|Age|Experience|Married/Single|House_Ownership|Car_Ownership|          Profession|               CITY|         STATE|CURRENT_JOB_YRS|CURRENT_HOUSE_YRS|Married/Single_encoded|House_Ownership_encoded|Car_Ownership_encoded|Profession_encoded|     CITY_encoded|  STATE_encoded|\n",
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+----------------------+-----------------------+---------------------+------------------+-----------------+---------------+\n",
            "|1303834| 23|         3|        single|         rented|           no| Mechanical_engineer|               Rewa|Madhya_Pradesh|              3|               13|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[11],[1.0])|(317,[151],[1.0])| (29,[6],[1.0])|\n",
            "|7574516| 40|        10|        single|         rented|           no|  Software_Developer|           Parbhani|   Maharashtra|              9|               13|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[20],[1.0])|(317,[119],[1.0])| (29,[1],[1.0])|\n",
            "|3991815| 66|         4|       married|         rented|           no|    Technical_writer|          Alappuzha|        Kerala|              4|               10|         (2,[1],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[13],[1.0])|(317,[228],[1.0])|(29,[14],[1.0])|\n",
            "|6256451| 41|         2|        single|         rented|          yes|  Software_Developer|        Bhubaneswar|        Odisha|              2|               12|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[1],[1.0])|   (51,[20],[1.0])|(317,[287],[1.0])|(29,[17],[1.0])|\n",
            "|5768871| 47|        11|        single|         rented|           no|       Civil_servant|Tiruchirappalli[10]|    Tamil_Nadu|              3|               14|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[48],[1.0])|(317,[143],[1.0])| (29,[5],[1.0])|\n",
            "|6915937| 64|         0|        single|         rented|           no|       Civil_servant|            Jalgaon|   Maharashtra|              0|               12|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[48],[1.0])|(317,[114],[1.0])| (29,[1],[1.0])|\n",
            "|3954973| 58|        14|       married|         rented|           no|           Librarian|           Tiruppur|    Tamil_Nadu|              8|               12|         (2,[1],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[42],[1.0])|(317,[128],[1.0])| (29,[5],[1.0])|\n",
            "|1706172| 33|         2|        single|         rented|           no|           Economist|           Jamnagar|       Gujarat|              2|               14|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[45],[1.0])|(317,[105],[1.0])| (29,[8],[1.0])|\n",
            "|7566849| 24|        17|        single|         rented|          yes|    Flight_attendant|            Kota[6]|     Rajasthan|             11|               11|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[1],[1.0])|   (51,[17],[1.0])|(317,[248],[1.0])| (29,[9],[1.0])|\n",
            "|8964846| 23|        12|        single|         rented|           no|           Architect|         Karimnagar|     Telangana|              5|               13|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[40],[1.0])|(317,[194],[1.0])|(29,[12],[1.0])|\n",
            "|4634680| 78|         7|        single|         rented|           no|    Flight_attendant|        Hajipur[31]|         Bihar|              7|               12|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[17],[1.0])|  (317,[7],[1.0])| (29,[4],[1.0])|\n",
            "|6623263| 22|         4|        single|         rented|           no|            Designer|              Adoni|Andhra_Pradesh|              4|               14|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[44],[1.0])| (317,[50],[1.0])| (29,[2],[1.0])|\n",
            "|9120988| 28|         9|        single|         rented|           no|           Physician|          Erode[17]|    Tamil_Nadu|              9|               12|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|    (51,[0],[1.0])| (317,[14],[1.0])| (29,[5],[1.0])|\n",
            "|8043880| 57|        12|        single|         rented|           no|   Financial_Analyst|             Kollam|        Kerala|              8|               10|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[15],[1.0])|(317,[193],[1.0])|(29,[14],[1.0])|\n",
            "|9420838| 48|         6|        single|         rented|           no|    Technical_writer|            Madurai|    Tamil_Nadu|              6|               10|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[13],[1.0])|(317,[182],[1.0])| (29,[5],[1.0])|\n",
            "|5694236| 39|         2|       married|         rented|          yes|           Economist|    Anantapuram[24]|Andhra_Pradesh|              2|               10|         (2,[1],[1.0])|          (3,[0],[1.0])|        (2,[1],[1.0])|   (51,[45],[1.0])| (317,[29],[1.0])| (29,[2],[1.0])|\n",
            "|7315840| 71|         8|       married|         rented|           no|Air_traffic_contr...|          Kamarhati|   West_Bengal|              8|               14|         (2,[1],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|    (51,[8],[1.0])|(317,[168],[1.0])| (29,[3],[1.0])|\n",
            "|3666346| 56|        12|        single|         rented|           no|          Politician|           Bhusawal|   Maharashtra|             12|               11|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[24],[1.0])|(317,[219],[1.0])| (29,[1],[1.0])|\n",
            "|2241112| 28|         8|        single|         rented|           no|      Police_officer|              Sirsa|       Haryana|              6|               14|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[22],[1.0])| (317,[86],[1.0])|(29,[11],[1.0])|\n",
            "|5431918| 40|         1|        single|         rented|           no|              Artist|          Amaravati|Andhra_Pradesh|              1|               14|         (2,[0],[1.0])|          (3,[0],[1.0])|        (2,[0],[1.0])|   (51,[27],[1.0])|(317,[123],[1.0])| (29,[2],[1.0])|\n",
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+----------------------+-----------------------+---------------------+------------------+-----------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "\n",
        "# Define the list of categorical columns\n",
        "categorical_cols = ['Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE']\n",
        "\n",
        "# Index categorical columns\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "\n",
        "# One-hot encode indexed categorical columns\n",
        "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_encoded\") for col in categorical_cols]\n",
        "\n",
        "# Assemble feature vector\n",
        "assembler_inputs = [col + \"_encoded\" for col in categorical_cols] + ['Income', 'Age', 'Experience', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS']\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "# Pipeline of indexers, encoders, and assembler\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "\n",
        "# Fit and transform the pipeline to preprocess the data\n",
        "X_preprocessed = pipeline.fit(X).transform(X)\n",
        "\n",
        "# Split the preprocessed data into training and testing sets (70% train, 30% test)\n",
        "train_data, test_data = X_preprocessed.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Show the counts of the training and testing data\n",
        "print(\"Training Data Count:\", train_data.count())\n",
        "print(\"Testing Data Count:\", test_data.count())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUr48zYOhd6P",
        "outputId": "f1f4e4b8-b29b-40d4-c1e9-0dcf24e1c977"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Count: 176421\n",
            "Testing Data Count: 75579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJycKxJUmb3W",
        "outputId": "e83ba319-7c4e-4094-a2a6-266f12ee4528"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Income: integer (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Married/Single: string (nullable = true)\n",
            " |-- House_Ownership: string (nullable = true)\n",
            " |-- Car_Ownership: string (nullable = true)\n",
            " |-- Profession: string (nullable = true)\n",
            " |-- CITY: string (nullable = true)\n",
            " |-- STATE: string (nullable = true)\n",
            " |-- CURRENT_JOB_YRS: integer (nullable = true)\n",
            " |-- CURRENT_HOUSE_YRS: integer (nullable = true)\n",
            " |-- Married/Single_index: double (nullable = false)\n",
            " |-- House_Ownership_index: double (nullable = false)\n",
            " |-- Car_Ownership_index: double (nullable = false)\n",
            " |-- Profession_index: double (nullable = false)\n",
            " |-- CITY_index: double (nullable = false)\n",
            " |-- STATE_index: double (nullable = false)\n",
            " |-- Married/Single_encoded: vector (nullable = true)\n",
            " |-- House_Ownership_encoded: vector (nullable = true)\n",
            " |-- Car_Ownership_encoded: vector (nullable = true)\n",
            " |-- Profession_encoded: vector (nullable = true)\n",
            " |-- CITY_encoded: vector (nullable = true)\n",
            " |-- STATE_encoded: vector (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Risk_Flag')\n",
        "\n",
        "# Train the model on the training data\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Risk_Flag')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ysXsa0LfmQl8",
        "outputId": "4466884d-71f0-462f-e993-facace4a2583"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute '_jdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-c4d931cca986>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Make predictions on the testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_jdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_dummies is a pandas DataFrame containing your feature columns\n",
        "\n",
        "# Convert X_dummies to a NumPy array\n",
        "X_array = X_dummies.values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WXAr_mL8hx9y"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming y is a PySpark Column\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "\n",
        "# Collect the values from the PySpark Column and convert them to a NumPy array\n",
        "y_array = np.array(y.collect())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "qDzwb-3zirbR",
        "outputId": "ef3db313-79f7-4496-ecf1-6b60d6805429"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'Column' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-b41732120ea6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Collect the values from the PySpark Column and convert them to a NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a8d2e7b-f70e-48e8-be5b-b22455a2ab38",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98nZ_0oKTLTl",
        "outputId": "26910650-5bb5-44ad-b62f-a0a21b19c81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+--------------+---------------+-------------+-------------------+-------------------+--------------+---------------+-----------------+\n",
            "| Income|Age|Experience|Married/Single|House_Ownership|Car_Ownership|         Profession|               CITY|         STATE|CURRENT_JOB_YRS|CURRENT_HOUSE_YRS|\n",
            "+-------+---+----------+--------------+---------------+-------------+-------------------+-------------------+--------------+---------------+-----------------+\n",
            "|1303834| 23|         3|        single|         rented|           no|Mechanical_engineer|               Rewa|Madhya_Pradesh|              3|               13|\n",
            "|7574516| 40|        10|        single|         rented|           no| Software_Developer|           Parbhani|   Maharashtra|              9|               13|\n",
            "|3991815| 66|         4|       married|         rented|           no|   Technical_writer|          Alappuzha|        Kerala|              4|               10|\n",
            "|6256451| 41|         2|        single|         rented|          yes| Software_Developer|        Bhubaneswar|        Odisha|              2|               12|\n",
            "|5768871| 47|        11|        single|         rented|           no|      Civil_servant|Tiruchirappalli[10]|    Tamil_Nadu|              3|               14|\n",
            "+-------+---+----------+--------------+---------------+-------------+-------------------+-------------------+--------------+---------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a new DataFrame X by selecting all columns except 'Risk_Flag'\n",
        "X = super_clean_df.select([col for col in super_clean_df.columns if col != 'Risk_Flag'])\n",
        "\n",
        "# Show the first few rows of the new DataFrame X\n",
        "X.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ad524744-3846-4e56-8337-8f59f1be3240",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MQKhKBBzTLTl",
        "outputId": "32537d15-9d44-42d9-dbf4-48868caeaa94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int, Risk_Flag: int, Married/Single_index: double, House_Ownership_index: double, Car_Ownership_index: double]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Define the columns to be encoded\n",
        "categorical_cols = ['Married/Single', 'House_Ownership', 'Car_Ownership']\n",
        "\n",
        "# Apply StringIndexer to each categorical column\n",
        "indexed_df = super_clean_df\n",
        "for col in categorical_cols:\n",
        "    indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(indexed_df)\n",
        "    indexed_df = indexer.transform(indexed_df)\n",
        "\n",
        "# Display the encoded DataFrame\n",
        "display(indexed_df)\n",
        "\n",
        "\n",
        "#We'll use StringIndexer to encode the categorical columns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cce04886-5481-4da8-8a3f-927690f422f3",
          "showTitle": false,
          "title": ""
        },
        "id": "Skp01wQJTLTl"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define features by combining encoded categorical columns and numerical columns\n",
        "feature_cols = [col + \"_index\" for col in categorical_cols] + ['Income', 'Age', 'Experience', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Define DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"Risk_Flag\", featuresCol=\"features\")\n",
        "\n",
        "# Create a Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, dt])\n",
        "\n",
        "# Fit the pipeline to the data\n",
        "model = pipeline.fit(indexed_df)\n",
        "\n",
        "# predictions = model.transform(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ade53847-df31-4e47-889d-d6da26f54ea5",
          "showTitle": false,
          "title": ""
        },
        "id": "UMao4R7gTLTm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "62ae232d-dc0c-4711-8697-10875c5a965a",
          "showTitle": false,
          "title": ""
        },
        "id": "xi6CH6FaTLTm"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "\n",
        "# Assuming you have already defined train_data and test_data\n",
        "\n",
        "# Define features by combining encoded categorical columns and numerical columns\n",
        "feature_cols = [col + \"_index\" for col in categorical_cols] + ['Income', 'Age', 'Experience', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS']\n",
        "\n",
        "# Assemble features with a different output column name\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
        "\n",
        "# Define StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "# Fit the pipeline on the training data and transform both training and testing data\n",
        "pipeline_model = pipeline.fit(train_data)\n",
        "train_data_scaled = pipeline_model.transform(train_data)\n",
        "test_data_scaled = pipeline_model.transform(test_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b0514c7b-1561-44ef-9bde-9e6f1bbe94e1",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzcbN4HoTLTm",
        "outputId": "1c52d3d2-5d4f-4870-a2d2-6a77c0f3604e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Income: integer (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Married/Single: string (nullable = true)\n",
            " |-- House_Ownership: string (nullable = true)\n",
            " |-- Car_Ownership: string (nullable = true)\n",
            " |-- Profession: string (nullable = true)\n",
            " |-- CITY: string (nullable = true)\n",
            " |-- STATE: string (nullable = true)\n",
            " |-- CURRENT_JOB_YRS: integer (nullable = true)\n",
            " |-- CURRENT_HOUSE_YRS: integer (nullable = true)\n",
            " |-- Married/Single_index: double (nullable = false)\n",
            " |-- House_Ownership_index: double (nullable = false)\n",
            " |-- Car_Ownership_index: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- assembled_features: vector (nullable = true)\n",
            " |-- scaled_features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data_scaled.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "24364c50-9680-483a-9eca-31fea883a820",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJUUhP2sTLTm",
        "outputId": "fde6ae24-d562-4e14-e0b8-2e61bdca59aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Income: integer (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Married/Single: string (nullable = true)\n",
            " |-- House_Ownership: string (nullable = true)\n",
            " |-- Car_Ownership: string (nullable = true)\n",
            " |-- Profession: string (nullable = true)\n",
            " |-- CITY: string (nullable = true)\n",
            " |-- STATE: string (nullable = true)\n",
            " |-- CURRENT_JOB_YRS: integer (nullable = true)\n",
            " |-- CURRENT_HOUSE_YRS: integer (nullable = true)\n",
            " |-- Married/Single_index: double (nullable = false)\n",
            " |-- House_Ownership_index: double (nullable = false)\n",
            " |-- Car_Ownership_index: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- assembled_features: vector (nullable = true)\n",
            " |-- scaled_features: vector (nullable = true)\n",
            "\n",
            "+------+---+----------+--------------+---------------+-------------+----------+---------+-----------+---------------+-----------------+--------------------+---------------------+-------------------+--------------------+--------------------+--------------------+\n",
            "|Income|Age|Experience|Married/Single|House_Ownership|Car_Ownership|Profession|     CITY|      STATE|CURRENT_JOB_YRS|CURRENT_HOUSE_YRS|Married/Single_index|House_Ownership_index|Car_Ownership_index|            features|  assembled_features|     scaled_features|\n",
            "+------+---+----------+--------------+---------------+-------------+----------+---------+-----------+---------------+-----------------+--------------------+---------------------+-------------------+--------------------+--------------------+--------------------+\n",
            "| 10310| 70|        20|        single|         rented|          yes|  Engineer|Baranagar|West_Bengal|              7|               14|                 0.0|                  0.0|                1.0|[0.0,0.0,1.0,1031...|[0.0,0.0,1.0,1031...|[0.0,0.0,2.179836...|\n",
            "| 10310| 70|        20|        single|         rented|          yes|  Engineer|Baranagar|West_Bengal|              7|               14|                 0.0|                  0.0|                1.0|[0.0,0.0,1.0,1031...|[0.0,0.0,1.0,1031...|[0.0,0.0,2.179836...|\n",
            "| 10310| 70|        20|        single|         rented|          yes|  Engineer|Baranagar|West_Bengal|              7|               14|                 0.0|                  0.0|                1.0|[0.0,0.0,1.0,1031...|[0.0,0.0,1.0,1031...|[0.0,0.0,2.179836...|\n",
            "| 10310| 70|        20|        single|         rented|          yes|  Engineer|Baranagar|West_Bengal|              7|               14|                 0.0|                  0.0|                1.0|[0.0,0.0,1.0,1031...|[0.0,0.0,1.0,1031...|[0.0,0.0,2.179836...|\n",
            "| 10310| 70|        20|        single|         rented|          yes|  Engineer|Baranagar|West_Bengal|              7|               14|                 0.0|                  0.0|                1.0|[0.0,0.0,1.0,1031...|[0.0,0.0,1.0,1031...|[0.0,0.0,2.179836...|\n",
            "+------+---+----------+--------------+---------------+-------------+----------+---------+-----------+---------------+-----------------+--------------------+---------------------+-------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the DataFrame schema\n",
        "train_data_scaled.printSchema()\n",
        "\n",
        "# Show some sample rows\n",
        "train_data_scaled.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into Train and Test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dummies, y, random_state=78)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "7aBWJablZ0P9",
        "outputId": "1498250c-cff9-405d-b6b9-20504a6cd763"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [1, 252000]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-99a8de6b032f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Splitting into Train and Test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dummies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 252000]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b90e793e-96f2-4591-96f0-09edede9a75c",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "FhfCBFJzTLTm",
        "outputId": "8d4d6015-03e0-4f5c-827b-f68f4853d9f5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Output column assembled_features already exists.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f6a427b3ac43>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Fit the pipeline to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Optionally, make predictions on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Output column assembled_features already exists."
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Define feature columns\n",
        "feature_cols = [col + \"_index\" for col in categorical_cols] + ['Income', 'Age', 'Experience', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS']\n",
        "\n",
        "# Create a VectorAssembler with a different output column name\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
        "\n",
        "# Import the Random Forest classifier\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Instantiate the Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"Risk_Flag\", featuresCol=\"assembled_features\", numTrees=100)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "rf_model = pipeline.fit(train_data_scaled)\n",
        "\n",
        "# Optionally, make predictions on test data\n",
        "predictions = rf_model.transform(test_data_scaled)\n",
        "\n",
        "# Optionally, evaluate the model performance\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Risk_Flag\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c8c3ff74-c982-4515-99da-e902d08c5f28",
          "showTitle": false,
          "title": ""
        },
        "id": "mzOSAGqATLTm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
          "showTitle": false,
          "title": ""
        },
        "id": "m2sRDlQdTLTm",
        "outputId": "25d63df9-6479-4fdc-bf3b-297a23d69a56"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"Training_Data__1__csv\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "implicitDf": true,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b5f66379-6f7f-42ec-8e82-d0e0926a1721",
          "showTitle": false,
          "title": ""
        },
        "id": "X7xrNBasTLTm",
        "outputId": "5b9bb5ed-6087-42ea-eebd-645b3a73a01d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "%sql\n",
        "\n",
        "/* Query the created temp table in a SQL cell */\n",
        "\n",
        "select * from `Training_Data__1__csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
          "showTitle": false,
          "title": ""
        },
        "id": "Os3JdC2yTLTm",
        "outputId": "62340e08-e0b8-4244-c00f-8e0d9c33f90d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n",
        "# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n",
        "# To do so, choose your table name and uncomment the bottom line.\n",
        "\n",
        "permanent_table_name = \"Training_Data__1__csv\"\n",
        "\n",
        "# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 2264462330196191,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "2024-03-06 - DBFS Example",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}