{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silversteinaustin/Machine-Learning-Credit-Score-Analysis/blob/main/2024_03_06_DBFS_Example_3_5_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
          "showTitle": false,
          "title": ""
        },
        "id": "wpu03pInTLTi"
      },
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
        "\n",
        "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bec92824-3fff-4309-98b1-9607a68c6902",
          "showTitle": false,
          "title": ""
        },
        "id": "895K69S4TLTk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score # AUC ROC\n",
        "from sklearn.metrics import average_precision_score # AUC PRC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YourAppName\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "EUu9W9B7UMiP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "psTNg6ikTLTj",
        "outputId": "dc6310d7-c0f0-4b12-b6e5-6328d281ecaa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int, Risk_Flag: int]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# File location and type\n",
        "file_location = \"/content/super_clean_df.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "loan_df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(loan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "713d8411-5fd8-4fe0-9127-bd2739d7a6a8",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNZzn2AoTLTk",
        "outputId": "4a9e0ec8-70ff-4b94-ea57-50a6c43dbe36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Id: int, Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int, Risk_Flag: int]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "loan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ebd794a-3df0-4658-8eba-11d1f592fa9c",
          "showTitle": false,
          "title": ""
        },
        "id": "T8APv5CETLTk"
      },
      "outputs": [],
      "source": [
        "#dropped the unnecessary data from out dataframe\n",
        "\n",
        "clean_df = loan_df.drop('ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7594663f-a112-4685-b5da-fab63b853381",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MsDAy0B5TLTk",
        "outputId": "26a99db9-ac09-4dd4-f133-c8f8d70c28a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int, Risk_Flag: int]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(clean_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e23ca235-d142-4f66-8354-7f726a2955e4",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKhacWoqTLTl",
        "outputId": "4815e989-4197-4ac5-e469-9d7dca60137b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "clean_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ef3f5927-3e18-46f8-a6b6-5942f2117cbc",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCEsp4yVTLTl",
        "outputId": "a10c5e99-545a-413f-c429-33ea50f3cf71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252000"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "super_clean_df = clean_df.dropna()\n",
        "super_clean_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "06f7d23e-c76a-4b9b-8814-80ad994f5533",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Zm8dN6TLTl",
        "outputId": "717c4607-4bfa-47d0-de20-ae47b1fb3bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Income', 'Age', 'Experience', 'Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS', 'Risk_Flag']\n"
          ]
        }
      ],
      "source": [
        "column_list = super_clean_df.columns\n",
        "print(column_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "30841d99-2cfb-47af-a39e-2c59cd8ca753",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlUIdNlhTLTl",
        "outputId": "d73652e9-c575-40a8-b8e2-1719057fa7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norent_noown\n",
            "rented\n",
            "owned\n"
          ]
        }
      ],
      "source": [
        "# Check the unique values in the House_Ownership column\n",
        "unique_house_ownership_values = super_clean_df.select('House_Ownership').distinct().collect()\n",
        "\n",
        "# Print the unique values\n",
        "for row in unique_house_ownership_values:\n",
        "    print(row['House_Ownership'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "847570d9-54dd-4b43-be6d-887f7579edfe",
          "showTitle": false,
          "title": ""
        },
        "id": "5Tg7lhLtTLTl"
      },
      "outputs": [],
      "source": [
        "# Define custom mapping for House_Ownership column\n",
        "house_ownership_mapping = {\"norent_noown\": 0, \"rented\": 1, \"owned\": 2}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1356c1dc-5ec9-4e24-a0ae-c134e2fdb820",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7gdQ3m5TLTm",
        "outputId": "07b66ed9-8e26-4b76-dc13-abdf898c8dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Extract the 'Risk_Flag' column from the DataFrame and convert it to a numpy array\n",
        "y = np.array(super_clean_df.select(\"Risk_Flag\").collect())\n",
        "\n",
        "# Drop the 'Risk_Flag' column from the DataFrame to get the feature matrix\n",
        "X = super_clean_df.drop(\"Risk_Flag\")\n",
        "\n",
        "# Display the first 5 elements of y\n",
        "print(y[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_list = super_clean_df.columns\n",
        "print(column_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuN7EcEnpW_L",
        "outputId": "c55364af-1398-4367-d9e0-ad4ad93fdef6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Income', 'Age', 'Experience', 'Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS', 'Risk_Flag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(super_clean_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "xSOhohw2Aiet",
        "outputId": "df72dfe1-a3d2-43c7-8819-4578c1cfcf41"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3A2CM0IfBVl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPWojQo6graZ",
        "outputId": "c9c4d9a7-dc42-4b79-aebd-19992d30637c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+\n",
            "| Income|Age|Experience|Married/Single|House_Ownership|Car_Ownership|          Profession|               CITY|         STATE|CURRENT_JOB_YRS|CURRENT_HOUSE_YRS|\n",
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+\n",
            "|1303834| 23|         3|        single|         rented|           no| Mechanical_engineer|               Rewa|Madhya_Pradesh|              3|               13|\n",
            "|7574516| 40|        10|        single|         rented|           no|  Software_Developer|           Parbhani|   Maharashtra|              9|               13|\n",
            "|3991815| 66|         4|       married|         rented|           no|    Technical_writer|          Alappuzha|        Kerala|              4|               10|\n",
            "|6256451| 41|         2|        single|         rented|          yes|  Software_Developer|        Bhubaneswar|        Odisha|              2|               12|\n",
            "|5768871| 47|        11|        single|         rented|           no|       Civil_servant|Tiruchirappalli[10]|    Tamil_Nadu|              3|               14|\n",
            "|6915937| 64|         0|        single|         rented|           no|       Civil_servant|            Jalgaon|   Maharashtra|              0|               12|\n",
            "|3954973| 58|        14|       married|         rented|           no|           Librarian|           Tiruppur|    Tamil_Nadu|              8|               12|\n",
            "|1706172| 33|         2|        single|         rented|           no|           Economist|           Jamnagar|       Gujarat|              2|               14|\n",
            "|7566849| 24|        17|        single|         rented|          yes|    Flight_attendant|            Kota[6]|     Rajasthan|             11|               11|\n",
            "|8964846| 23|        12|        single|         rented|           no|           Architect|         Karimnagar|     Telangana|              5|               13|\n",
            "|4634680| 78|         7|        single|         rented|           no|    Flight_attendant|        Hajipur[31]|         Bihar|              7|               12|\n",
            "|6623263| 22|         4|        single|         rented|           no|            Designer|              Adoni|Andhra_Pradesh|              4|               14|\n",
            "|9120988| 28|         9|        single|         rented|           no|           Physician|          Erode[17]|    Tamil_Nadu|              9|               12|\n",
            "|8043880| 57|        12|        single|         rented|           no|   Financial_Analyst|             Kollam|        Kerala|              8|               10|\n",
            "|9420838| 48|         6|        single|         rented|           no|    Technical_writer|            Madurai|    Tamil_Nadu|              6|               10|\n",
            "|5694236| 39|         2|       married|         rented|          yes|           Economist|    Anantapuram[24]|Andhra_Pradesh|              2|               10|\n",
            "|7315840| 71|         8|       married|         rented|           no|Air_traffic_contr...|          Kamarhati|   West_Bengal|              8|               14|\n",
            "|3666346| 56|        12|        single|         rented|           no|          Politician|           Bhusawal|   Maharashtra|             12|               11|\n",
            "|2241112| 28|         8|        single|         rented|           no|      Police_officer|              Sirsa|       Haryana|              6|               14|\n",
            "|5431918| 40|         1|        single|         rented|           no|              Artist|          Amaravati|Andhra_Pradesh|              1|               14|\n",
            "+-------+---+----------+--------------+---------------+-------------+--------------------+-------------------+--------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxHvPdmEkdHS",
        "outputId": "01302cfe-7f86-4c58-cc4e-951fb56326af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "eb2a4df6-b14a-4df3-8d2e-7a029401d2e9",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "o10NvEXvTLTl",
        "outputId": "ff4797a1-b4a3-4cd0-b0eb-e7c6895115dc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'to_pandas'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-52df6bb06666>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_dummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_dummies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_pandas'"
          ]
        }
      ],
      "source": [
        "pd_test = X.to_pandas()\n",
        "X_dummies = pd.get_dummies(pd_test)\n",
        "X_dummies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "# from pyspark.ml import Pipeline\n",
        "\n",
        "# # List of categorical columns\n",
        "# categorical_cols = ['Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE']\n",
        "\n",
        "# # Index categorical columns\n",
        "# indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "\n",
        "# # One-hot encode indexed categorical columns\n",
        "# encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_encoded\") for col in categorical_cols]\n",
        "\n",
        "# # Pipeline of indexers and encoders\n",
        "# pipeline = Pipeline(stages=indexers + encoders)\n",
        "\n",
        "# # Fit and transform the pipeline to create dummy variables\n",
        "# X_encoded = pipeline.fit(X).transform(X)\n",
        "\n",
        "# # Drop the original categorical columns\n",
        "# X_encoded = X_encoded.drop(*[col+\"_index\" for col in categorical_cols])\n",
        "\n",
        "# # Show the resulting DataFrame with dummy variables\n",
        "# X_encoded.show()\n"
      ],
      "metadata": {
        "id": "LK74K-JXlDxk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=ps.get_dummies(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "XdQnjR4H6ajP",
        "outputId": "5b4ed386-1310-4f09-ca90-4cb87172b694"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute '_internal'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-890aecf47532>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/pandas/namespace.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m   2219\u001b[0m             column_labels = [\n\u001b[1;32m   2220\u001b[0m                 \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2221\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpsdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2222\u001b[0m                 if isinstance(\n\u001b[1;32m   2223\u001b[0m                     \u001b[0mpsdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_type_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_dummies_default_accept_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_internal'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "XJycKxJUmb3W",
        "outputId": "ecf1e981-d76d-40f0-b1ec-a06f34e785a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int]\n",
              "0                                                  1                                                                                                                                                                       "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59a52476-fc1d-4377-96bf-c45ce75883c4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59a52476-fc1d-4377-96bf-c45ce75883c4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59a52476-fc1d-4377-96bf-c45ce75883c4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59a52476-fc1d-4377-96bf-c45ce75883c4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_ada2ed75-2543-493c-b316-81c17e85ed5c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ada2ed75-2543-493c-b316-81c17e85ed5c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"DataFrame[Income: int, Age: int, Experience: int, Married/Single: string, House_Ownership: string, Car_Ownership: string, Profession: string, CITY: string, STATE: string, CURRENT_JOB_YRS: int, CURRENT_HOUSE_YRS: int]\",\n      \"properties\": {\n        \"dtype\": \"uint8\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "SGKjC-Z89xOk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DataFrame to CSV\n",
        "super_clean_df.toPandas().to_csv('super_clean_df.csv', index=False)\n"
      ],
      "metadata": {
        "id": "oVjkaG0x9ZFK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Risk_Flag')\n",
        "\n",
        "# Train the model on the training data\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Risk_Flag')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ysXsa0LfmQl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_dummies is a pandas DataFrame containing your feature columns\n",
        "\n",
        "# Convert X_dummies to a NumPy array\n",
        "X_array = X_dummies.values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WXAr_mL8hx9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming y is a PySpark Column\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "\n",
        "# Collect the values from the PySpark Column and convert them to a NumPy array\n",
        "y_array = np.array(y.collect())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qDzwb-3zirbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a8d2e7b-f70e-48e8-be5b-b22455a2ab38",
          "showTitle": false,
          "title": ""
        },
        "id": "98nZ_0oKTLTl"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a new DataFrame X by selecting all columns except 'Risk_Flag'\n",
        "X = super_clean_df.select([col for col in super_clean_df.columns if col != 'Risk_Flag'])\n",
        "\n",
        "# Show the first few rows of the new DataFrame X\n",
        "X.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ad524744-3846-4e56-8337-8f59f1be3240",
          "showTitle": false,
          "title": ""
        },
        "id": "MQKhKBBzTLTl"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Define the columns to be encoded\n",
        "categorical_cols = ['Married/Single', 'House_Ownership', 'Car_Ownership']\n",
        "\n",
        "# Apply StringIndexer to each categorical column\n",
        "indexed_df = super_clean_df\n",
        "for col in categorical_cols:\n",
        "    indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(indexed_df)\n",
        "    indexed_df = indexer.transform(indexed_df)\n",
        "\n",
        "# Display the encoded DataFrame\n",
        "display(indexed_df)\n",
        "\n",
        "\n",
        "#We'll use StringIndexer to encode the categorical columns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cce04886-5481-4da8-8a3f-927690f422f3",
          "showTitle": false,
          "title": ""
        },
        "id": "Skp01wQJTLTl"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define features by combining encoded categorical columns and numerical columns\n",
        "feature_cols = [col + \"_index\" for col in categorical_cols] + ['Income', 'Age', 'Experience', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Define DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"Risk_Flag\", featuresCol=\"features\")\n",
        "\n",
        "# Create a Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, dt])\n",
        "\n",
        "# Fit the pipeline to the data\n",
        "model = pipeline.fit(indexed_df)\n",
        "\n",
        "# predictions = model.transform(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ade53847-df31-4e47-889d-d6da26f54ea5",
          "showTitle": false,
          "title": ""
        },
        "id": "UMao4R7gTLTm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "62ae232d-dc0c-4711-8697-10875c5a965a",
          "showTitle": false,
          "title": ""
        },
        "id": "xi6CH6FaTLTm"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "\n",
        "# Assuming you have already defined train_data and test_data\n",
        "\n",
        "# Define features by combining encoded categorical columns and numerical columns\n",
        "feature_cols = [col + \"_index\" for col in categorical_cols] + ['Income', 'Age', 'Experience', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS']\n",
        "\n",
        "# Assemble features with a different output column name\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
        "\n",
        "# Define StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "# Fit the pipeline on the training data and transform both training and testing data\n",
        "pipeline_model = pipeline.fit(train_data)\n",
        "train_data_scaled = pipeline_model.transform(train_data)\n",
        "test_data_scaled = pipeline_model.transform(test_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b0514c7b-1561-44ef-9bde-9e6f1bbe94e1",
          "showTitle": false,
          "title": ""
        },
        "id": "zzcbN4HoTLTm"
      },
      "outputs": [],
      "source": [
        "train_data_scaled.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "24364c50-9680-483a-9eca-31fea883a820",
          "showTitle": false,
          "title": ""
        },
        "id": "pJUUhP2sTLTm"
      },
      "outputs": [],
      "source": [
        "# Display the DataFrame schema\n",
        "train_data_scaled.printSchema()\n",
        "\n",
        "# Show some sample rows\n",
        "train_data_scaled.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into Train and Test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dummies, y, random_state=78)\n"
      ],
      "metadata": {
        "id": "7aBWJablZ0P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b90e793e-96f2-4591-96f0-09edede9a75c",
          "showTitle": false,
          "title": ""
        },
        "id": "FhfCBFJzTLTm"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Define feature columns\n",
        "feature_cols = [col + \"_index\" for col in categorical_cols] + ['Income', 'Age', 'Experience', 'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS']\n",
        "\n",
        "# Create a VectorAssembler with a different output column name\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
        "\n",
        "# Import the Random Forest classifier\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Instantiate the Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"Risk_Flag\", featuresCol=\"assembled_features\", numTrees=100)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "rf_model = pipeline.fit(train_data_scaled)\n",
        "\n",
        "# Optionally, make predictions on test data\n",
        "predictions = rf_model.transform(test_data_scaled)\n",
        "\n",
        "# Optionally, evaluate the model performance\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Risk_Flag\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c8c3ff74-c982-4515-99da-e902d08c5f28",
          "showTitle": false,
          "title": ""
        },
        "id": "mzOSAGqATLTm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
          "showTitle": false,
          "title": ""
        },
        "id": "m2sRDlQdTLTm"
      },
      "outputs": [],
      "source": [
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"Training_Data__1__csv\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "implicitDf": true,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b5f66379-6f7f-42ec-8e82-d0e0926a1721",
          "showTitle": false,
          "title": ""
        },
        "id": "X7xrNBasTLTm"
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "\n",
        "/* Query the created temp table in a SQL cell */\n",
        "\n",
        "select * from `Training_Data__1__csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
          "showTitle": false,
          "title": ""
        },
        "id": "Os3JdC2yTLTm"
      },
      "outputs": [],
      "source": [
        "# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n",
        "# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n",
        "# To do so, choose your table name and uncomment the bottom line.\n",
        "\n",
        "permanent_table_name = \"Training_Data__1__csv\"\n",
        "\n",
        "# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 2264462330196191,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "2024-03-06 - DBFS Example",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}